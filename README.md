# Guide to Springboard Capstones, Tutorials and Case Studies

![image](https://user-images.githubusercontent.com/86930309/233186316-14d6a5d3-ac53-4225-896d-691c50f22848.png)

# Capstones

1. [Ski Ticket Price Predictor](https://github.com/GHASS19/Ski_Ticket_Price_Predictor_Capstone)

### Main Ideas

- Test two models to predict the price of a ski ticket for an adult on the weekend at Big Moutain Ski Resort in Montana.

- I used cross validation in linear regression and random forest to predict the price.

- The random forest model was more accurate. It had a lower mean absolute error by over $1 then the other model.

### Key Findings

- Fast Quad lifts, the amount of ski runs, snow making and vertical drop in feet were the most important features to predicting the price in both the linear regression and random forrest models.

- Big Mountain offers more facilities and skiable terrain than most other resorts in the United States.

- In both of our models vertical drop, snow making, total chairs, fast quads, runs, longest run, trams and skiable terrain were the most important to guests.

- The resort is in the top portion of fast quads, runs, snow making, total chairs and vertical drop compared to other places.

### Recommendations

- Big Mountain Resort should increase their weekend price to $95.87 from $81.00.

- I would do an experiment for a certain amount of time and close just one ski run on the weekend to reduce maintenance cost.

- Adding one run, 150 vertical feet and a new chairlift, (fast quad) is what I highly suggest the resort enact. This plan justifies a ticket price increase of $1.99 that improves revenue to $3,474,638 per year.

- Big Mountain needs to invest in facilities that guests value the most on their ski trips.

- The resort needs data on maintenance cost to make a solid decision going forward.

# Tutorials

## I. [Intro to Gradient Boosting](https://github.com/GHASS19/Intro_to_Gradient_Boosting)

### Main Objectives:

 - Understand the conceptual difference between bagging and boosting ensembles.

- Understand how gradient boosting works for regression tasks.

- Learn how to tune the key hyperparameters of gradient boosting ensembles.

## II. [Data Cleaning Exercise](https://github.com/GHASS19/Data_Cleaning_Exercise)

 ### Main Objectives:
 
- Was to clean the  climbing data to prepare it for modeling and machine learning.

- What to do with NaN values.

## III. [Automated Feature Engineering](https://github.com/GHASS19/Feature_Engineering)

### Main Objectives:

- Using automated feature engineering as to build hundreds or thousands of relevant features from a relational dataset.

- Calculating a feature matrix with several hundred relevant features for predicting customer churn.

- Ensuring that our features are made with valid data for each cutoff time.

## IV. [Real Estate Investment Plan](https://github.com/GHASS19/Real_Estate_Investment_Plan)

### Main Objectives:

- Using a real estate database, find a state for an ivestment property with optimum traits.

- Create graphs in matplotlib to analyze where to purchase the investment property.

- Based upon the data the best place to purchase was the state was Massachusettes. That had a four bed and two bathroom home for less than the mean price.

# Case Studies

## 1. [Bayesian Optimization LightGBM](https://github.com/GHASS19/Bayesian_Optimization_LightGBM_Case_Study)

### Main Ideas:

- Predict if a flight departure is going to be delayed by 15 minutes based on the variables and then find the best results.

- Learn how Bayesian Optimization works with a graph of the Gaussian process.

- Test the Bayesian optimization on real flight departures data using the Light GBM.

## 2. [Linear Regression Red Wine Study](https://github.com/GHASS19/Linear-Regression-Case-Study-of-the-Red-Wine-Dataset)

### Main Ideas:

-  I used linear regression to predict the fixed acidity of red wine using just one variable and then mulitple variables.

- Load and Source the red wine data.

- Exploratory Data Analysis. Displaying heatmaps, pairplot and a few scatterplots.

- Linear Regression Modeling. Our best model was 4. It had an R2 score of .742 and used fewer predictors. 

## 3. [Cowboy Cigarettes Time Series ARIMA](https://github.com/GHASS19/Cowboy_Cigarettes_Time_Series_Case_Study)

### Main Ideas:

- Use the 1949-1960 data to predict the manufacturer's cigarette sales after they stopped in 1960.

- Sourcing and loading the cigarette data. Cleaning, transforming and visualizing or dataset.

- I made the data stationary to prepare it for the ARIMA model. 

- The best p,d, q parameters for our ARIMA model were 2, 1, 1.

- The ARIMA model predicted cigarette sales starting in December of 1960. 

- I concluded that people purchased more cigarettes during the summer possibly due to the good weather, disposable income and time off.

## 4. [Grid Search in K-Nearest Neighbor Model Case Study](https://github.com/GHASS19/Grid-Search-in-KNN-Model-Case-Study)

### Main Ideas:

- Utilized KNN with 31 different neighbors in predicting if a Pima Indian had diabetes or not. 

- The KNN model had an accuracy score of .752644. This was better than the random forest model used as well. 

- This was a Classification problem in which I used cross validation, precision, recall and f1-score to measure model preformance. 

## 5. 
